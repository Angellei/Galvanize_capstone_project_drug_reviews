{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.proportion import proportions_chisquare\n",
    "from scipy.stats import chisquare\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## only need to remove punctuation and stemize\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"web_scrap.csv\",index_col=0)\n",
    "df.columns = ['first','Overall rating','Effectiveness','Side effects','Condition','Dosage','Other conditions','Other drugs taken','Benefits','Detailed Side effects','Comments']\n",
    "tem = df['first'].str.split(\" \",expand=True)[[0,4,7]]\n",
    "tem.columns = ['name','age','gender']\n",
    "df = pd.concat([tem,df],1).drop('first',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Side effects\n",
       " Extremely Severe Side Effects     500\n",
       " Mild Side Effects                2559\n",
       " Moderate Side Effects            1637\n",
       " No Side Effects                  2330\n",
       " Severe Side Effects               932\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Side effects').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Side effects Yes/No'] = (df['Side effects'] != ' No Side Effects').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jzz0026/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "con_vec = TfidfVectorizer(stop_words='english',tokenizer=tokenize)\n",
    "X_train = con_vec.fit_transform(train['Comments'])\n",
    "y_train = train['Side effects Yes/No']\n",
    "\n",
    "X_test = con_vec.transform(test['Comments'])\n",
    "y_test = test['Side effects Yes/No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73768844, 0.74610357, 0.7340372 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2')\n",
    "lr_cv_score = cross_val_score(lr,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "lr_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7678392 , 0.76973353, 0.75465058])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin = SVC(kernel='linear')\n",
    "svm_lin_cv_score = cross_val_score(svm_lin,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "svm_lin_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70603015, 0.70638512, 0.70638512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin = SVC(kernel='rbf')\n",
    "svm_lin_cv_score = cross_val_score(svm_lin,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "svm_lin_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84623116, 0.84464555, 0.83660131])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=800,n_jobs=-1)\n",
    "rfc_cv_score = cross_val_score(rfc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "rfc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.77195175e-07, 7.99636998e-06, 3.52125225e-06, ...,\n",
       "       3.12009485e-06, 3.49801123e-06, 2.53551888e-06])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=800,n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day                 0.012572\n",
       "onc                 0.010308\n",
       "tablet              0.009960\n",
       "pill                0.009482\n",
       "daili               0.009462\n",
       "morn                0.009191\n",
       "1                   0.006762\n",
       "took                0.005866\n",
       "time                0.005693\n",
       "twice               0.005627\n",
       "treatment           0.005612\n",
       "hour                0.005436\n",
       "taken               0.005352\n",
       "befor               0.005281\n",
       "need                0.005278\n",
       "everi               0.005064\n",
       "year                0.004677\n",
       "mg                  0.004666\n",
       "effect              0.004657\n",
       "week                0.004527\n",
       "night               0.004270\n",
       "dosag               0.004089\n",
       "just                0.004061\n",
       "month               0.003830\n",
       "stop                0.003483\n",
       "use                 0.003435\n",
       "bed                 0.003416\n",
       "drug                0.003403\n",
       "thyroid             0.003389\n",
       "onli                0.003292\n",
       "                      ...   \n",
       "aarp                0.000000\n",
       "meunfunct           0.000000\n",
       "tentat              0.000000\n",
       "metoprolol          0.000000\n",
       "tequin              0.000000\n",
       "methotrexciatesp    0.000000\n",
       "97                  0.000000\n",
       "absolult            0.000000\n",
       "tank                0.000000\n",
       "switchand           0.000000\n",
       "competitor          0.000000\n",
       "mouthblood          0.000000\n",
       "sx                  0.000000\n",
       "coma                0.000000\n",
       "commend             0.000000\n",
       "moreov              0.000000\n",
       "moremi              0.000000\n",
       "moodier             0.000000\n",
       "monthswhil          0.000000\n",
       "ace                 0.000000\n",
       "misdemeanor         0.000000\n",
       "mono                0.000000\n",
       "tackl               0.000000\n",
       "completelythey      0.000000\n",
       "molar               0.000000\n",
       "mmt                 0.000000\n",
       "conclud             0.000000\n",
       "talika              0.000000\n",
       "miseri              0.000000\n",
       "fadedand            0.000000\n",
       "Name: 0, Length: 8096, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = pd.DataFrame(rfc.feature_importances_,con_vec.get_feature_names())\n",
    "importance[0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72110553, 0.72900955, 0.71644042])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc_cv_score = cross_val_score(gbc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
