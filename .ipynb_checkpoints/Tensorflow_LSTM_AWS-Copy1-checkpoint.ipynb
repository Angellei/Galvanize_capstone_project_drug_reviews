{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "#import requests\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "## remove special symbol\n",
    "def rm_sym(df):\n",
    "    df['review'] = df['review'].str.replace(\"&#039;\",'\\'')\n",
    "    df['review'].head()\n",
    "    df['rating_cate'] = ''\n",
    "    df.loc[df['rating'] >= 7,'rating_cate'] = 'high'\n",
    "    df.loc[df['rating'] <= 4,'rating_cate'] = 'low'\n",
    "    df.loc[(df['rating'] > 4) & (df['rating'] < 7),'rating_cate'] = 'medium'\n",
    "    return df\n",
    "\n",
    "def clean_text(df_tem3):\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\\\r)|(\\\\n)|(\\\\t)|(\\\\f)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(&#039;)|(\\d\\s)|(\\d)|(\\/)\",\"\")\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\$)|(\\-)|(\\\\)|(\\s{2,})\",\" \")\n",
    "    df_tem3['review'].sample(1).iloc[0]\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df_tem3['review'] = df_tem3['review'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split(\" \")]))\n",
    "    return df_tem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drugsCom_raw/drugsComTrain_raw.tsv',sep='\\t',index_col=0).sample(20000)\n",
    "df = rm_sym(df)\n",
    "df_tem3 = df\n",
    "\n",
    "test = pd.read_csv(\"drugsCom_raw/drugsComTest_raw.tsv\",sep='\\t', index_col=0)\n",
    "test = rm_sym(test)\n",
    "\n",
    "df_tem3 = clean_text(df_tem3)\n",
    "test = clean_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating_cate\n",
       "high      13259\n",
       "low        4996\n",
       "medium     1745\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tem3.groupby('rating_cate').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 7)\n",
      "(53766, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df_tem3.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "\n",
    "#from tensorflow import tensorflow.keras\n",
    "\n",
    "#from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D \n",
    "\n",
    "# fix random seed for reproducibility\n",
    "\n",
    "MAX_NB_WORDS = 500\n",
    "max_review_length = 500\n",
    "EMBEDDING_DIM = 160\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True, split=' ', char_level=False, \n",
    "                      oov_token=None, document_count=0)\n",
    "\n",
    "tokenizer.fit_on_texts(df_tem3['review'])\n",
    "train_sequences = tokenizer.texts_to_sequences(df_tem3['review'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen = max_review_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157993</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27559</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69596</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168467</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        high  low  medium\n",
       "157993     0    1       0\n",
       "27559      0    1       0\n",
       "69596      1    0       0\n",
       "168467     0    1       0\n",
       "19615      0    1       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform y to get_dummies\n",
    "y_train = pd.get_dummies(df_tem3['rating_cate'])\n",
    "y_test = pd.get_dummies(test['rating_cate'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 500) <-- shape of train_data ready for val/train split.\n",
      "(53766, 500) <-- shape of final_test_data ready for fedding to network.\n",
      "18754 <-- Length of Word Index\n"
     ]
    }
   ],
   "source": [
    "# Print shapes of data. \n",
    "\n",
    "print(X_train.shape, '<-- shape of train_data ready for val/train split.')\n",
    "print(X_test.shape, '<-- shape of final_test_data ready for fedding to network.')\n",
    "print(len(tokenizer.word_index), '<-- Length of Word Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train and validation data by dividing train_data in 80:20 ratio\n",
      "train data shape: (16000, 500)\n",
      "validation data shape: (4000, 500)\n",
      "Data is ready for training!!\n"
     ]
    }
   ],
   "source": [
    "# Split Training & Validation Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print('creating train and validation data by dividing train_data in 80:20 ratio')\n",
    "######################################################\n",
    "\n",
    "X_train_t, X_train_val, Y_train_t, y_train_val = train_test_split(X_train, y_train,test_size = 0.2)\n",
    "\n",
    "######################################################\n",
    "print('train data shape:', X_train_t.shape)\n",
    "print('validation data shape:', X_train_val.shape)\n",
    "print('Data is ready for training!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up Model Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words  = min(MAX_NB_WORDS, len(word_index))\n",
    "lstm_out = max_review_length\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,EMBEDDING_DIM,input_length=max_review_length))\n",
    "\n",
    "## add conv using kernal No.32 and size 3x3, actiation='relu'(rm neg)\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(LSTM(50))\n",
    "\n",
    "#model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 160)          80000     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 500, 32)           15392     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 112,145\n",
      "Trainable params: 112,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug_Data_LSTM_w_stopwords_1.0_trainging_cycle1batchsize_32\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/40\n",
      "16000/16000 [==============================] - 115s 7ms/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.6738 - val_acc: 0.8294\n",
      "Epoch 2/40\n",
      "10624/16000 [==================>...........] - ETA: 35s - loss: 0.0819 - acc: 0.9693"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Run LSTM Model\n",
    "batch = 32 \n",
    "epoch = 40\n",
    "\n",
    "## set name for the mdoel\n",
    "training_cycle = 1\n",
    "notebookname = \"Drug_Data_\"\n",
    "variant = \"LSTM_w_stopwords_\"\n",
    "version = \"1.0_\"\n",
    "title = notebookname + variant + version\n",
    "\n",
    "stamp = '{}trainging_cycle{}batchsize_{}'.format(title,training_cycle,batch)\n",
    "print(stamp)\n",
    "\n",
    "## save the best model\n",
    "best_model_path = title + stamp + 'best.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only = True) ## save only best model\n",
    "\n",
    "## if 4 steps without decreasing of loss in valid set, stop the trainning\n",
    "early_stopping = EarlyStopping(patience = 4)\n",
    "\n",
    "LSTM_model = model.fit(X_train_t, Y_train_t, batch_size=batch, epochs=epoch,\n",
    "                       validation_data=(X_train_val, y_train_val),callbacks=[early_stopping], shuffle = True)\n",
    "\n",
    "best_score = min(LSTM_model.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1000\n",
    "[0.7633334,\n",
    " 0.765,\n",
    " 0.7733334,\n",
    " 0.7766667,\n",
    " 0.79,\n",
    " 0.78333336,\n",
    " 0.77833337,\n",
    " 0.77833337,\n",
    " 0.78666663]\n",
    "\n",
    "## 10000\n",
    "[0.7896667,\n",
    " 0.78500015,\n",
    " 0.79650015,\n",
    " 0.7953333,\n",
    " 0.7994998,\n",
    " 0.79933333,\n",
    " 0.79466665,\n",
    " 0.79899997,\n",
    " 0.79566664,\n",
    " 0.7911668,\n",
    " 0.8008332,\n",
    " 0.8033334,\n",
    " 0.7893331,\n",
    " 0.8063332,\n",
    " 0.80533326,\n",
    " 0.8041667]\n",
    "\n",
    "## 20000\n",
    "[0.8406666798591613,\n",
    " 0.8463333468437195,\n",
    " 0.8442500100135804,\n",
    " 0.8465000066757202,\n",
    " 0.845666675567627,\n",
    " 0.8399166750907898]\n",
    "\n",
    "## 40000\n",
    "[0.846000010728836,\n",
    " 0.8528333415985108,\n",
    " 0.8522500121593475,\n",
    " 0.8555416769981384,\n",
    " 0.829625009059906,\n",
    " 0.8536250035762787,\n",
    " 0.8491666755676269]\n",
    "\n",
    "## all\n",
    "[0.8746228654859085,\n",
    " 0.8851105690445873,\n",
    " 0.889078329655966,\n",
    " 0.8945443354995075,\n",
    " 0.896011581290825,\n",
    " 0.8991320621279996,\n",
    " 0.8992973819550564,\n",
    " 0.9017668994849171,\n",
    " 0.9046290611673865,\n",
    " 0.908173181695814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accr = model.evaluate(X_test,y_test, batch_size = 100)\n",
    "accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10000\n",
    "## [0.4519780118446444, 0.8386464840319391]\n",
    "\n",
    "## 40000\n",
    "[0.3983093709805641, 0.8546789216107478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.values.argmax(axis=1),y_test_pred)\n",
    "## 10000\n",
    "## 0.7498047092958375\n",
    "\n",
    "## 40000\n",
    "## 0.7756574787040137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test.values.argmax(axis=1),y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
