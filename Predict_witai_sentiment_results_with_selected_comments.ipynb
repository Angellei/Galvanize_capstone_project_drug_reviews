{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wit import Wit\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Selected_comments_with_witai_sentiment_results\n",
    "## import comment with side effects for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SE_tem_l400 = pd.read_csv(\"wit_ai_sentment/SE_tem_l400.csv\",index_col=0,header=None)[1]\n",
    "SE_tem_l400 = SE_tem_l400.iloc[3:].astype(str)\n",
    "idxs = list(SE_tem_l400.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quest2 = pd.read_csv(\"wit_ai_sentment/SE_tem_l400_first_request.ccv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 30689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "value\n",
       "negative    16765\n",
       "neutral     12010\n",
       "positive     1914\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total results:\",len(first_quest2))\n",
    "first_quest2.groupby('value').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I haven't noticed any major side effects\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_quest2[first_quest2['value'] == 'negative'].sample(1)['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model using only these comment with side effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## only need to remove punctuation and stemize\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## remove special symbol\n",
    "def rm_sym(df):\n",
    "    df['review'] = df['review'].str.replace(\"&#039;\",'\\'')\n",
    "    df['review'].head()\n",
    "    df['rating_cate'] = ''\n",
    "    df.loc[df['rating'] >= 7,'rating_cate'] = 'high'\n",
    "    df.loc[df['rating'] <= 4,'rating_cate'] = 'low'\n",
    "    df.loc[(df['rating'] > 4) & (df['rating'] < 7),'rating_cate'] = 'medium'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('drugsCom_raw/drugsComTrain_raw.tsv',sep='\\t',index_col=0)\n",
    "df = rm_sym(df)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"drugsCom_raw/drugsComTest_raw.tsv\",sep='\\t', index_col=0)\n",
    "test = rm_sym(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jzz0026/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([df.loc[first_quest2.index], first_quest2],1)\n",
    "## Generate table of words with their counts\n",
    "con_vec = TfidfVectorizer(stop_words='english',tokenizer=tokenize)\n",
    "X_train_p = con_vec.fit_transform(train['review'])\n",
    "#y_train = train['rating_cate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30689,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negnornot = (train['value'] == \"negative\").astype(int)\n",
    "negnornot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test =  train_test_split(X_train_p,negnornot,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67770103, 0.68013556, 0.67579194])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1',multi_class='auto',solver='saga')\n",
    "#lr.fit(X,target_3.iloc[:,0])\n",
    "\n",
    "cv_score = cross_val_score(lr,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75381207, 0.75677789, 0.75609438])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300,n_jobs=-1)\n",
    "rfc_cv_score = cross_val_score(rfc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "rfc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8239280594291672"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=300,n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_test_predict = rfc.predict(X_test)\n",
    "accuracy_score(y_test,y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.010434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect</th>\n",
       "      <td>0.009831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headach</th>\n",
       "      <td>0.005854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horribl</th>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.005247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.005210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.004526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onli</th>\n",
       "      <td>0.004459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.004272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>0.004171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nausea</th>\n",
       "      <td>0.004009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug</th>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.003903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ani</th>\n",
       "      <td>0.003755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribl</th>\n",
       "      <td>0.003562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.003552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri</th>\n",
       "      <td>0.003534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ive</th>\n",
       "      <td>0.003470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.003468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>becaus</th>\n",
       "      <td>0.003457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pain</th>\n",
       "      <td>0.003419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medic</th>\n",
       "      <td>0.003404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.003401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.003364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.003347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sever</th>\n",
       "      <td>0.003269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im</th>\n",
       "      <td>0.003267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>0.003198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itburn</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150160</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jist</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jigg</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jcv</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jawneck</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japan</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symptomat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>janumet1000x2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jantoven</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jangl</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i️d</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i️</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iveri</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ivc</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symptomsterrifi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iudwhich</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itthought</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itther</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>synarel</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itraconazol</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itquoti</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itpharmacist</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itmad</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itliter</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itimmedi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ithorr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>synovi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itchingpain</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laterno</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24462 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "bad              0.010434\n",
       "effect           0.009831\n",
       "headach          0.005854\n",
       "horribl          0.005587\n",
       "year             0.005247\n",
       "work             0.005210\n",
       "day              0.004526\n",
       "onli             0.004459\n",
       "month            0.004272\n",
       "week             0.004171\n",
       "nausea           0.004009\n",
       "drug             0.003911\n",
       "start            0.003903\n",
       "ani              0.003755\n",
       "terribl          0.003562\n",
       "feel             0.003552\n",
       "veri             0.003534\n",
       "ive              0.003470\n",
       "time             0.003468\n",
       "becaus           0.003457\n",
       "pain             0.003419\n",
       "medic            0.003404\n",
       "great            0.003401\n",
       "tri              0.003383\n",
       "worst            0.003364\n",
       "like             0.003347\n",
       "sever            0.003269\n",
       "im               0.003267\n",
       "2                0.003238\n",
       "just             0.003198\n",
       "...                   ...\n",
       "itburn           0.000000\n",
       "150160           0.000000\n",
       "jist             0.000000\n",
       "jigg             0.000000\n",
       "jcv              0.000000\n",
       "jawneck          0.000000\n",
       "japan            0.000000\n",
       "symptomat        0.000000\n",
       "janumet1000x2    0.000000\n",
       "jantoven         0.000000\n",
       "jangl            0.000000\n",
       "i️d              0.000000\n",
       "i️               0.000000\n",
       "iveri            0.000000\n",
       "ivc              0.000000\n",
       "symptomsterrifi  0.000000\n",
       "iudwhich         0.000000\n",
       "itthought        0.000000\n",
       "itther           0.000000\n",
       "synarel          0.000000\n",
       "itraconazol      0.000000\n",
       "itquoti          0.000000\n",
       "itpharmacist     0.000000\n",
       "itmad            0.000000\n",
       "itliter          0.000000\n",
       "itimmedi         0.000000\n",
       "ithorr           0.000000\n",
       "synovi           0.000000\n",
       "itchingpain      0.000000\n",
       "laterno          0.000000\n",
       "\n",
       "[24462 rows x 1 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_,con_vec.get_feature_names())\n",
    "feature_importances.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68943047, 0.69642857, 0.6876548 ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=300)\n",
    "gbc_cv_score = cross_val_score(gbc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54659195, 0.54666319, 0.54660409])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_poly = SVC(kernel='poly')\n",
    "gbc_cv_score = cross_val_score(svm_poly,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6909944 , 0.69121481, 0.68309217])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_lin = SVC(kernel='linear')\n",
    "gbc_cv_score = cross_val_score(svm_lin,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
