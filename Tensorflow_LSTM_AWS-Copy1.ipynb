{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "#import requests\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "## remove special symbol\n",
    "def rm_sym(df):\n",
    "    df['review'] = df['review'].str.replace(\"&#039;\",'\\'')\n",
    "    df['review'].head()\n",
    "    df['rating_cate'] = ''\n",
    "    df.loc[df['rating'] >= 7,'rating_cate'] = 'high'\n",
    "    df.loc[df['rating'] <= 4,'rating_cate'] = 'low'\n",
    "    df.loc[(df['rating'] > 4) & (df['rating'] < 7),'rating_cate'] = 'medium'\n",
    "    return df\n",
    "\n",
    "def clean_text(df_tem3):\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\\\r)|(\\\\n)|(\\\\t)|(\\\\f)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(&#039;)|(\\d\\s)|(\\d)|(\\/)\",\"\")\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\$)|(\\-)|(\\\\)|(\\s{2,})\",\" \")\n",
    "    df_tem3['review'].sample(1).iloc[0]\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df_tem3['review'] = df_tem3['review'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split(\" \")]))\n",
    "    return df_tem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drugsCom_raw/drugsComTrain_raw.tsv',sep='\\t',index_col=0)#.sample(40000)\n",
    "df = rm_sym(df)\n",
    "df_tem3 = df\n",
    "\n",
    "test = pd.read_csv(\"drugsCom_raw/drugsComTest_raw.tsv\",sep='\\t', index_col=0)\n",
    "test = rm_sym(test)\n",
    "\n",
    "df_tem3 = clean_text(df_tem3)\n",
    "test = clean_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tem3.groupby('rating_cate').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tem3.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "\n",
    "#from tensorflow import tensorflow.keras\n",
    "\n",
    "#from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D \n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "\n",
    "MAX_NB_WORDS = 500\n",
    "max_review_length = 500\n",
    "EMBEDDING_DIM = 160\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True, split=' ', char_level=False, \n",
    "                      oov_token=None, document_count=0)\n",
    "\n",
    "tokenizer.fit_on_texts(df_tem3['review'])\n",
    "train_sequences = tokenizer.texts_to_sequences(df_tem3['review'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen = max_review_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y to get_dummies\n",
    "y_train = pd.get_dummies(df_tem3['rating_cate'])\n",
    "y_test = pd.get_dummies(test['rating_cate'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of data. \n",
    "\n",
    "print(X_train.shape, '<-- shape of train_data ready for val/train split.')\n",
    "print(X_test.shape, '<-- shape of final_test_data ready for fedding to network.')\n",
    "print(len(tokenizer.word_index), '<-- Length of Word Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Training & Validation Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print('creating train and validation data by dividing train_data in 80:20 ratio')\n",
    "######################################################\n",
    "\n",
    "X_train_t, X_train_val, Y_train_t, y_train_val = train_test_split(X_train, y_train,test_size = 0.2)\n",
    "\n",
    "######################################################\n",
    "print('train data shape:', X_train_t.shape)\n",
    "print('validation data shape:', X_train_val.shape)\n",
    "print('Data is ready for training!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up Model Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words  = min(MAX_NB_WORDS, len(word_index))\n",
    "lstm_out = max_review_length\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,EMBEDDING_DIM,input_length=max_review_length))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "## add conv using kernal No.32 and size 3x3, actiation='relu'(rm neg)\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#model.add(Attention(MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dense(3, activation = 'softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Run LSTM Model\n",
    "batch = 32 \n",
    "epoch = 40\n",
    "\n",
    "## set name for the mdoel\n",
    "training_cycle = 1\n",
    "notebookname = \"Drug_Data_\"\n",
    "variant = \"LSTM_w_stopwords_\"\n",
    "version = \"1.0_\"\n",
    "title = notebookname + variant + version\n",
    "\n",
    "stamp = '{}trainging_cycle{}batchsize_{}'.format(title,training_cycle,batch)\n",
    "print(stamp)\n",
    "\n",
    "## save the best model\n",
    "best_model_path = title + stamp + 'best.h5'\n",
    "model_checkpoint = ModelCheckpoint(best_model_path, save_best_only = True) ## save only best model\n",
    "\n",
    "## if 4 steps without decreasing of loss in valid set, stop the trainning\n",
    "early_stopping = EarlyStopping(patience = 4)\n",
    "\n",
    "LSTM_model = model.fit(X_train_t, Y_train_t, batch_size=batch, epochs=epoch,\n",
    "                       validation_data=(X_train_val, y_train_val),callbacks=[early_stopping], shuffle = True)\n",
    "\n",
    "best_score = min(LSTM_model.history['val_loss'])\n",
    "\n",
    "## why difference between train and val loss could be randomly different? local optima?\n",
    "## add dropout to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LSTM_model.history['loss'],label='train')\n",
    "plt.plot(LSTM_model.history['val_loss'],label='validation')\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first try\n",
    "## 1000\n",
    "[0.7633334,\n",
    " 0.765,\n",
    " 0.7733334,\n",
    " 0.7766667,\n",
    " 0.79,\n",
    " 0.78333336,\n",
    " 0.77833337,\n",
    " 0.77833337,\n",
    " 0.78666663]\n",
    "\n",
    "## 10000\n",
    "[0.7896667,\n",
    " 0.78500015,\n",
    " 0.79650015,\n",
    " 0.7953333,\n",
    " 0.7994998,\n",
    " 0.79933333,\n",
    " 0.79466665,\n",
    " 0.79899997,\n",
    " 0.79566664,\n",
    " 0.7911668,\n",
    " 0.8008332,\n",
    " 0.8033334,\n",
    " 0.7893331,\n",
    " 0.8063332,\n",
    " 0.80533326,\n",
    " 0.8041667]\n",
    "\n",
    "## 20000\n",
    "[0.8406666798591613,\n",
    " 0.8463333468437195,\n",
    " 0.8442500100135804,\n",
    " 0.8465000066757202,\n",
    " 0.845666675567627,\n",
    " 0.8399166750907898]\n",
    "\n",
    "## 40000\n",
    "[0.846000010728836,\n",
    " 0.8528333415985108,\n",
    " 0.8522500121593475,\n",
    " 0.8555416769981384,\n",
    " 0.829625009059906,\n",
    " 0.8536250035762787,\n",
    " 0.8491666755676269]\n",
    "\n",
    "## all\n",
    "[0.8746228654859085,\n",
    " 0.8851105690445873,\n",
    " 0.889078329655966,\n",
    " 0.8945443354995075,\n",
    " 0.896011581290825,\n",
    " 0.8991320621279996,\n",
    " 0.8992973819550564,\n",
    " 0.9017668994849171,\n",
    " 0.9046290611673865,\n",
    " 0.908173181695814]\n",
    "\n",
    "\n",
    "## add convential layer and dropout(0.2)\n",
    "## 20000\n",
    "[0.8463333458900452,\n",
    " 0.8495833463668824,\n",
    " 0.8557500090599061,\n",
    " 0.8566666769981385,\n",
    " 0.858500009059906,\n",
    " 0.8403333406448364,\n",
    " 0.8505833492279052,\n",
    " 0.8565000033378601]\n",
    "\n",
    "## 40000\n",
    "[0.8470000097751618,\n",
    " 0.8522500097751617,\n",
    " 0.8575416767597198,\n",
    " 0.8592500100135804,\n",
    " 0.8623750069141388,\n",
    " 0.8616666796207428,\n",
    " 0.861833342552185,\n",
    " 0.86329167842865]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accr = model.evaluate(X_test,y_test, batch_size = 100)\n",
    "accr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10000\n",
    "## [0.4519780118446444, 0.8386464840319391]\n",
    "\n",
    "## 40000\n",
    "[0.3983093709805641, 0.8546789216107478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test#.values#.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test.values.argmax(axis=1),y_test_pred)\n",
    "## 10000\n",
    "## 0.7498047092958375\n",
    "\n",
    "## 40000\n",
    "## 0.7756574787040137\n",
    "\n",
    "## second try\n",
    "## 40000\n",
    "## 0.7865007625637019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test.values.argmax(axis=1),y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
